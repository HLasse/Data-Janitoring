---
title: "Assignment 1 - Language Development in ASD - part 3"
author: "Riccardo Fusaroli"
date: "August 10, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to the third exciting part of the Language Development in ASD exercise

In this exercise we will delve more in depth with different practices of model comparison and model selection, by first evaluating your models from last time, then learning how to cross-validate models and finally how to systematically compare models.

N.B. There are several datasets for this exercise, so pay attention to which one you are using!

1. The (training) dataset from last time (the awesome one you produced :-) ).
2. The (test) datasets on which you can test the models from last time:
* Demographic and clinical data: https://www.dropbox.com/s/ra99bdvm6fzay3g/demo_test.csv?dl=0
* Utterance Length data: https://www.dropbox.com/s/uxtqqzl18nwxowq/LU_test.csv?dl=0
* Word data: https://www.dropbox.com/s/1ces4hv8kh0stov/token_test.csv?dl=0

### Exercise 1) Testing model performance

How did your models from last time perform? In this exercise you have to compare the results on the training data () and on the test data. Report both of them. Compare them. Discuss why they are different.

- recreate the models you chose last time (just write the code again and apply it to Assignment2TrainData1.csv)
- calculate performance of the model on the training data: root mean square error is a good measure. (Tip: google the functions rmse() and predict() )
- create the test dataset (apply the code from assignment 1 part 1 to clean up the 3 test datasets)
- test the performance of the models on the test data (Tips: time to reuse "predict()")
- optional: predictions are never certain, can you identify the uncertainty of the predictions? (e.g. google predictinterval())


#####Answer
The RMSE of the model (CHI_MLU ~ Visit + MOT_MLU + verbalIQ + (1+Visit±ID) on the trainingdata was 0.335.
When tested on the test data however, the RMSE rose to 0.700. 

###Recreating model from last time and calculating model performance through RMSE
```{r}
p_load(tidyverse, lme4, lmerTest, stringr, modelr, MuMIn, caret, Metrics, merTools)
library(groupdata2)
#Recreating model from last exercise
#reading training data
trainingdata = read.csv("AutismWithAvg.csv", sep =",")
trainingdata$X = NULL
trainingdata$ID = as.numeric(trainingdata$ID)
trainingdata$Visit = as.numeric(trainingdata$Visit)

m1 = lmer(CHI_MLU ~ Visit + MOT_MLU + verbalIQ + (1+Visit|ID), trainingdata, REML =F) 

modelr::rmse(m1, trainingdata)
Metrics::rmse(trainingdata$CHI_MLU, predict(m1, trainingdata, allow.new.levels = T))
```

###Cleaning files
```{r}
# 
# #Reading files
# demdata = read.csv("demo_test.csv", sep=',')
# uttdata = read.csv("LU_test.csv", sep = ',')
# worddata = read.csv("token_test.csv", sep =',')
# 
# #Making "Visit" the consistent variable name
# names(uttdata)[names(uttdata)=="VISIT"] = "Visit"
# names(worddata)[names(worddata)=="VISIT"] = "Visit"
# #Making "ID" the consistent variable name
# names(demdata)[names(demdata)=="Child.ID"] = "ID"
# names(uttdata)[names(uttdata)=="SUBJ"] = "ID"
# names(worddata)[names(worddata)=="SUBJ"] = "ID"
# 
# #Using str_extract and regular expressions to only keep digits in the Visit colomns 
# uttdata$Visit = str_extract(uttdata$Visit, "\\d")
# worddata$Visit = str_extract(worddata$Visit, "\\d")
# 
# #Using gsub to remove all dots
# demdata$ID = gsub("\\.", "", demdata$ID)
# uttdata$ID = gsub("\\.", "", uttdata$ID)
# worddata$ID = gsub("\\.", "", worddata$ID)
# 
# #Using select to create subsets of the data containing only the wanted variables
# demsubdata = select(demdata, ID, Visit, Ethnicity, Diagnosis, Gender, Age, ADOS, MullenRaw, ExpressiveLangRaw)
# uttsubdata = select(uttdata, ID, Visit, MOT_MLU, MOT_LUstd, CHI_MLU, CHI_LUstd)
# wordsubdata = select(worddata, ID, Visit, types_MOT, types_CHI, tokens_MOT, tokens_CHI)
# 
# #Renaming MullenRaw and ExpressiveLangRaw
# names(demsubdata)[names(demsubdata)=="MullenRaw"] = "nonVerbalIQ"
# names(demsubdata)[names(demsubdata)=="ExpressiveLangRaw"] = "verbalIQ"
# 
# #Merging datasets
# utandword = merge(uttsubdata, wordsubdata, by = c("ID", "Visit"))
# joineddata = merge(utandword, demsubdata, by = c("ID", "Visit"))
# 
# #Creating subset with only the first visit
# visit1 = subset(joineddata[joineddata$Visit == "1",])
# #Selecting only the relevant variables
# visit1filter = select(visit1, ID, ADOS, nonVerbalIQ, verbalIQ)
# #Removing the variables from the joined dataset
# joineddata2 = joineddata[-15:-17]
# #Merging the two datasets 
# joineddata3 = merge(joineddata2, visit1filter, by = "ID")
# 
# ##Anonomyzing ID by enumerating it
# #Making ID a factor
# joineddata3$ID = as.factor(joineddata3$ID)
# #Renaming the levels as 1 through length of levels
# levels(joineddata3$ID) = 1:length(levels(joineddata3$ID))
# 
# #Making Visit numeric 
# joineddata3$Visit = as.numeric(joineddata3$Visit)
# 
# #Making Gender a factor and renaming the levels
# joineddata3$Gender = as.factor(joineddata3$Gender)
# 
# joineddata3$Gender = recode(joineddata3$Gender, "1" = "M", "2" = "F")
# 
# 
# #Renaming the factors of Diagnosis
# joineddata3$Diagnosis = recode(joineddata3$Diagnosis, "A"="ASD", "B"="TD")
# 
# #Writing test data
# #write.csv(joineddata3, file = "Autismtestdata.csv")

```

###Testing model predictions on the newly cleaned test data
```{r}
testdata = read.csv("Autismtestdata.csv", sep = ",")

modelr::rmse(m1, testdata)
Metrics::rmse(testdata$CHI_MLU, predict(m1, testdata, allow.new.levels = T))
#0.7 - both working the same so far

#Getting prediction intervals
intervalFit = predictInterval(m1, newdata = testdata)
intervalFit

```


### Exercise 2) Model Selection via Cross-validation (N.B: ChildMLU!)

One way to reduce bad surprises when testing a model on new data is to train the model via cross-validation. 

In this exercise you have to use cross-validation to calculate the predictive error of your models and use this predictive error to select the best possible model.

- Create the basic model of ChildMLU as a function of Time and Diagnosis (don't forget the random effects!).
- Make a cross-validated version of the model. (Tips: google the function "createFolds";  loop through each fold, train a model on the other folds and test it on the fold)
- Report the results and comment on them.

- Now try to find the best possible predictive model of ChildMLU, that is, the one that produces the best cross-validated results.

- Bonus Question 1: How would you go comparing the performance of the basic model and the cross-validated model on the testing set?
- Bonus Question 2: What is the effect of changing the number of folds? Can you plot RMSE as a function of number of folds?
- Bonus Question 3: compare the cross-validated predictive error against the actual predictive error on the test data

#####Answer
A basic model of CHI_MLU ~ Visit + Diagnosis with random intercepts for ID and random slopes for visit was tested using 4-fold cross-validation. Model performance on the training data was RMSE = 0.34, R²M = 0.22, R²C = 0.80.
Cross-validated performance was: Mean RMSE = ~ 0.81, mean R²M = 0.22, mean R²C = 0.80.

The cross-validated model has a far higher RMSE indicating that the model does not generalize as well as the uncross-validated results might lead one to think.


A model including Visit, Diagnosis, verbalIQ and types_CHI as fixed effects as well as Visit and ID as random effects seems to perform best when cross-validated. There are issues of strongly correlated effects in the model though. Mean RMSE of the model is around 0.45, mean R²M = 0.74, mean R²C = 0.85.

Model performance was also tested on the test data. For the basic (Visit and Diagnosis) model, performance was: RMSE = 1.07, R²m = 0.22, R²C = 0.80. For the best performing cross-validated model, performance was: RMSE = 0.58, R²M = 0.75, R²C = 0.85.

###Creating and testing the basic CHI_MLU ~ Visit + Diagnosis model using cross-validation
```{r}
#creating model and obtaining RMSE and R² 
m2 = lmer(CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID), trainingdata, REML = F)
modelr::rmse(m2, trainingdata)
r.squaredGLMM(m2)

#cv1 <- crossv_kfold(trainingdata, 4)
#folds = createFolds(trainingdata ,4)

nfolds = 4
trainingdatafold= fold(trainingdata, k=nfolds, cat_col = "Diagnosis", id_col = "ID")

#trainingdatafold = fold(trainingdata, k=4)
rmselist = list()
r2list = list()

for (i in 1:nfolds) {
  train = trainingdatafold[trainingdatafold$.folds != i, ]          #set the training set
  validation = trainingdatafold[trainingdatafold$.folds == i,]      #set the validation set
  newlm = lmer(CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID), train, REML = F) #creating the lmer
  rmselist[i] = Metrics::rmse(validation$CHI_MLU, predict(newlm, validation, allow.new.levels = T))  #saving model rmse
  r2list[i] = as.data.frame(r.squaredGLMM(newlm))                   #saving r2c and r2m
  r2list = as.data.frame(r2list)
}

rmselist
unlist(rmselist) %>% mean()


#Function to calculate the mean R2c and R2m across folds
#There has to be an easier way..   (the cv function helped..)
r2ListToDF = function(r2list) {
  r2df = as.data.frame(t(as.data.frame(r2list)))
  colnames(r2df) = c("R2m", "R2c")
  rownames(r2df) = seq(1:4)
  r2df = as.data.frame(r2df)
  print("Mean R2c")
  print(mean(r2df$R2c))
  print("Mean R2m")
  print(mean(r2df$R2m))
}
r2ListToDF(r2list)
```

###Creating function for crossvalidation
```{r}
#Creating function for automatic crossvalidation. Outputs R2c, R2m and RMSE for each fold, as well the mean values across folds
cv = function(data, k, model, dependent){
#Creating variables for storing performances
rmselist = list()
r2list = list()
#Creating loop
for (i in 1:k){
  train = data[data$.folds != i,]    #creating training set (all folds except the one)
  validation = data[data$.folds == i,] #creating testing/validation set (the current fold)
  model = lmer(model, train, REML = F)   #running lmer on the model specified in the function call
  rmselist[i] = Metrics::rmse(validation[[dependent]], predict(model, validation, allow.new.levels = T))  #saving model rmse
  r2list[i] = as.data.frame(r.squaredGLMM(model))     #saving r2c and r2m
}
#doing some wrangling so the R2 outputs can be printed in a nice format
r2list = as.data.frame(t(as.data.frame(r2list)))
colnames(r2list) = c("R2m", "R2c")
rownames(r2list) = seq(1:k)
r2list = as.data.frame(r2list)

#returning the wanted values
return(c('RMSE' = rmselist, 'Mean RMSE' = mean(unlist(rmselist)), r2list,  'Mean R2m' = mean(r2list$R2m), 'Mean R2c' =  mean(r2list$R2c)))
}

```

###Applying the function to test the performance of various models
```{r}
m1 = "CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID)"
cv(trainingdatafold, nfolds, m1, 'CHI_MLU')


###Testing different models
#Visit, Diagnosis and verbalIQ as predictors
m2 = "CHI_MLU ~ Visit + Diagnosis + verbalIQ + (1+Visit|ID)"
cv(trainingdatafold, nfolds, m2, 'CHI_MLU')
#mean rmse = 0.64
#mean R2C = 0.80
#mean R2m = 0.52

#Visit, MOT_MLU and verbalIQ as predictors
m3 = "CHI_MLU ~ Visit + MOT_MLU + verbalIQ + (1+Visit|ID)"
cv(trainingdatafold, nfolds, m3, 'CHI_MLU')
#mean rmse = 0.60
#mean R2c = 0.81
#mean R2m = 0.56

#Visit, Diagnosis, types_CHI and verbalIQ as predictors
m4 = "CHI_MLU ~ Visit + Diagnosis + verbalIQ + types_CHI + (1+Visit|ID)"
cv(trainingdatafold, nfolds, m4, 'CHI_MLU')
#mean rmse = 0.46
#mean R2c = 0.86
#mean R2m = 0.75 

#Visit * Diagnosis, tokens_CHI and verbalIQ as predictors
m5 = "CHI_MLU ~ Visit * Diagnosis + verbalIQ + tokens_CHI + (1+Visit|ID)"
cv(trainingdatafold, nfolds, m5, 'CHI_MLU')
#mean rmse = 0.44
#mean R2c = 0.86
#mean R2m = 0.78
```

```{r}
#Testing models on the test data
m1 = lmer(CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID), trainingdata, REML = F)
modelr::rmse(m1, testdata)
r.squaredGLMM(m1)

m4 = lmer(CHI_MLU ~ Visit + Diagnosis + verbalIQ + types_CHI + (1+Visit|ID), trainingdata, REML = F)
modelr::rmse(m4, testdata)
r.squaredGLMM(m4)

```

### Exercise 3) Assessing the single child

Let's get to business. This new kiddo - Bernie - has entered your clinic. This child has to be assessed according to his group's average and his expected development.

Bernie is one of the six kids in the test dataset, so make sure to extract that child alone for the following analysis.

You want to evaluate:

- how does the child fare in ChildMLU compared to the average TD child at each visit? Define the distance in terms of absolute difference between this Child and the average TD.
(Tip: recreate the equation of the model: Y=Intercept+BetaX1+BetaX2, etc; input the average of the TD group  for each parameter in the model as X1, X2, etc.).

- how does the child fare compared to the model predictions at Visit 6? Is the child below or above expectations? (tip: use the predict() function on Bernie's data only and compare the prediction with the actual performance of the child)

###Testing Bernie 
```{r}
#Bernie is the child with ID 2 
bernie = subset(testdata, ID == "2")

bernie = dplyr::select(bernie, ID, Visit, Diagnosis, verbalIQ, MOT_MLU, CHI_MLU)
bernie$CHI_MLU
bernie$ID = rep("Bernie", 6)

m = lmer(CHI_MLU ~ Visit + Diagnosis + verbalIQ + MOT_MLU + (1+Visit|ID), trainingdata)
summary(m)

m2 = lmer(CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID), trainingdata)
summary(m2)

tdsub = subset(trainingdata, Diagnosis == "TD")

#calculating mean MOT_MLU for parents of typically developing children
avgMOTMLU = tdsub %>% 
  group_by(Visit) %>% 
  summarize(MOT_MLU = mean(MOT_MLU))

#calculating mean CHI_MLU for children with autism
avgCHI_MLU = tdsub %>%
  group_by(Visit) %>%
  summarize(CHI_MLU = mean(CHI_MLU))
avgMOTMLU[2]
avgCHI_MLU

#calculating the difference between Bernie and the average child
diff = bernie$CHI_MLU - avgCHI_MLU[2]
diff %>% kable()

#Creating dataframe with the mean values for TD children
td_chi = data.frame(ID = rep("avgTD", 6), Visit = seq(1,6), Diagnosis = "TD", verbalIQ = mean(tdsub$verbalIQ), MOT_MLU = avgMOTMLU[2], CHI_MLU = avgCHI_MLU[2])


bernie$ID = as.factor(bernie$ID)
View(bernie)
#combining the Bernie data frame and the mean TD data frame
avgTD_bernie = rbind(td_chi, bernie)

predict(m, bernie, allow.new.levels = T)

#Using the model to obtain predicted values for Bernie
predictBernie = bernie
predictBernie$CHI_MLU = predict(m, bernie, allow.new.levels = T)
predictBernie$ID = rep("predictBernie", 6)
#Adding predicting values to the dataframe
avgTD_bernie = rbind(avgTD_bernie, predictBernie)

#Plotting the three children
ggplot(avgTD_bernie, aes(Visit, CHI_MLU, color = ID)) +
  geom_point() +
  geom_line()

sub6 = subset(avgTD_bernie, Visit == "6")
sub6
```


### OPTIONAL: Exercise 4) Model Selection via Information Criteria
Another way to reduce the bad surprises when testing a model on new data is to pay close attention to the relative information criteria between the models you are comparing. Let's learn how to do that!

Re-create a selection of possible models explaining ChildMLU (the ones you tested for exercise 2, but now trained on the full dataset and not cross-validated).

Then try to find the best possible predictive model of ChildMLU, that is, the one that produces the lowest information criterion.

- Bonus question for the optional exercise: are information criteria correlated with cross-validated RMSE? That is, if you take AIC for Model 1, Model 2 and Model 3, do they co-vary with their cross-validated RMSE?

### OPTIONAL: Exercise 5): Using Lasso for model selection
Welcome to the last secret exercise. If you have already solved the previous exercises, and still there's not enough for you, you can expand your expertise by learning about penalizations. Check out this tutorial: http://machinelearningmastery.com/penalized-regression-in-r/ and make sure to google what penalization is, with a focus on L1 and L2-norms. Then try them on your data!


